{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensorflow version 2.3.1\npandas 1.0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import feature_column\n",
    "from matplotlib import pyplot as plt\n",
    "# Serves to check that the Tensor Flow version is correct and imported correctly\n",
    "print('tensorflow version', tf.__version__)\n",
    "print('pandas', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "1            0        17.99         10.38          122.80     1001.0   \n",
       "2            0        20.57         17.77          132.90     1326.0   \n",
       "3            0        19.69         21.25          130.00     1203.0   \n",
       "4            0        11.42         20.38           77.58      386.1   \n",
       "5            0        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "565          0        21.56         22.39          142.00     1479.0   \n",
       "566          0        20.13         28.25          131.20     1261.0   \n",
       "567          0        16.60         28.08          108.30      858.1   \n",
       "568          0        20.60         29.33          140.10     1265.0   \n",
       "569          1         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "1            0.11840           0.27760         0.30010              0.14710   \n",
       "2            0.08474           0.07864         0.08690              0.07017   \n",
       "3            0.10960           0.15990         0.19740              0.12790   \n",
       "4            0.14250           0.28390         0.24140              0.10520   \n",
       "5            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "565          0.11100           0.11590         0.24390              0.13890   \n",
       "566          0.09780           0.10340         0.14400              0.09791   \n",
       "567          0.08455           0.10230         0.09251              0.05302   \n",
       "568          0.11780           0.27700         0.35140              0.15200   \n",
       "569          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  fractal_dimension_mean  \n",
       "1           0.2419                 0.07871  \n",
       "2           0.1812                 0.05667  \n",
       "3           0.2069                 0.05999  \n",
       "4           0.2597                 0.09744  \n",
       "5           0.1809                 0.05883  \n",
       "..             ...                     ...  \n",
       "565         0.1726                 0.05623  \n",
       "566         0.1752                 0.05533  \n",
       "567         0.1590                 0.05648  \n",
       "568         0.2397                 0.07016  \n",
       "569         0.1587                 0.05884  \n",
       "\n",
       "[569 rows x 11 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>symmetry_mean</th>\n      <th>fractal_dimension_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>0</td>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>0</td>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>0</td>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>0</td>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n    </tr>\n    <tr>\n      <th>569</th>\n      <td>1</td>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows Ã— 11 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "# Read in data and create a dataframe.\n",
    "# Create a columns array\n",
    "columns = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n",
    "columns_float = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n",
    "# Later we will need to split the data set into validation, test and training\n",
    "df = pd.read_csv('BreastCancerDataMeans.csv', names=columns, index_col=False)\n",
    "# Drop the ID column which we do not need, as well as the redundant headings\n",
    "df = df.drop('id', 1)\n",
    "df = df.drop(0)\n",
    "# Need to convert the object values into floats and the diagnosis into a string\n",
    "df['diagnosis'] = df.apply(lambda x: np.array(x['diagnosis']).astype(str), axis=1)\n",
    "for i in range(len(columns_float)):\n",
    "    df[columns_float[i]] = df.apply(lambda x: np.array(x[columns_float[i]]).astype(float), axis=1)\n",
    "df.head()\n",
    "# Need to convert the Malignant or Bengin result into a binary value\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 0, 'B':1})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data. Use a 75% training and 25% test split\n",
    "# test_size is how much of the data will be used for testing\n",
    "# random_state controls the shuffling to tae place, by defining the random state we can reproduce the same split of the data across multiple function calls\n",
    "# shuffle, we will enable this as the data may be organised in a certain way which could affect our results\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42, shuffle=True)\n",
    "# Define path for test and training data to be saved and convert them to CSV\n",
    "train_path = Path('train.tsv')\n",
    "test_path = Path('test.tsv')\n",
    "train_df.to_csv(train_path, sep='\\t', index=False)\n",
    "test_df.to_csv(test_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data over all the values, besides for target value\n",
    "# Calculate the z score\n",
    "# All rows, columns between 1 and 5\n",
    "#Normalising\n",
    "train_df_norm = train_df.iloc[:,1:5].sub(train_df_mean, axis=1)\n",
    "train_df_norm = train_df_norm.div(train_df_std, axis=1)\n",
    "\n",
    "#reassigning to dataframe\n",
    "#if rerunning, need to reimport data. the drop function permenantly changes dataframe meaning it can only operate once\n",
    "train_df_temp = train_df\n",
    "train_df_temp = train_df_temp.drop(columns = [\"radius_mean\",  \"texture_mean\",  \"perimeter_mean\",  \"area_mean\"]) #colums 1,2,3,4\n",
    "train_df = pd.concat([train_df_temp, train_df_norm], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     diagnosis  smoothness_mean  compactness_mean  concavity_mean  \\\n288          1          0.06955           0.03729         0.02260   \n513          0          0.11060           0.14690         0.14450   \n403          1          0.07351           0.07899         0.04057   \n447          0          0.09997           0.13140         0.16980   \n211          0          0.09090           0.13480         0.16400   \n..         ...              ...               ...             ...   \n72           1          0.09783           0.15310         0.08606   \n107          1          0.11420           0.10170         0.07070   \n271          1          0.06429           0.02675         0.00725   \n436          0          0.10600           0.11330         0.11260   \n103          1          0.08013           0.04038         0.02383   \n\n     concave points_mean  symmetry_mean  fractal_dimension_mean  radius_mean  \\\n288              0.01171         0.1337                 0.05581    -0.207600   \n513              0.08172         0.2116                 0.07325    -0.199177   \n403              0.01883         0.1874                 0.05899    -0.206444   \n447              0.08293         0.1713                 0.05916    -0.127332   \n211              0.09561         0.1765                 0.05024    -0.080591   \n..                   ...            ...                     ...          ...   \n72               0.02872         0.1902                 0.08980    -0.273698   \n107              0.03485         0.1801                 0.06520    -0.228246   \n271              0.00625         0.1508                 0.05376    -0.184478   \n436              0.06463         0.1669                 0.06544    -0.189598   \n103              0.01770         0.1739                 0.05677    -0.219327   \n\n     texture_mean  perimeter_mean  area_mean  \n288     -0.203802        0.932012   8.100175  \n513     -0.081582        1.043496   8.774033  \n403     -0.118413        0.969834   8.253775  \n447      0.042454        1.516848  15.791735  \n211     -0.054826        1.804229  20.885308  \n..            ...             ...        ...  \n72      -0.178697        0.550490   3.609441  \n107     -0.117753        0.821024   6.392408  \n271     -0.142692        1.070913  10.027607  \n436     -0.096447        1.084456   9.480924  \n103     -0.081582        0.854882   7.155453  \n\n[426 rows x 11 columns]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_df_std' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-f2f12466510d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_df_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_df_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#reassigning to dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df_std' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_df)\n",
    "test_df_norm = test_df.iloc[:,1:5].sub(test_df_mean, axis=1)\n",
    "test_df_norm = test_df_norm.div(test_df_std, axis=1)\n",
    "\n",
    "#reassigning to dataframe\n",
    "#if rerunning, need to reimport data. the drop function permenantly changes dataframe meaning it can only operate once\n",
    "test_df_temp = test_df\n",
    "test_df_temp = test_df_temp.drop(columns = [\"radius_mean\",  \"texture_mean\",  \"perimeter_mean\",  \"area_mean\"])\n",
    "test_df = pd.concat([test_df_temp, test_df_norm], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureColumns = []\n",
    "featureColumns.append(tf.feature_column.numeric_column('radius_mean'))\n",
    "featureColumns.append(tf.feature_column.numeric_column('texture_mean'))\n",
    "featureLayer = layers.DenseFeatures(featureColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(myLearningRate, featureLayer, myMetrics):\n",
    "    # Most simple tf.keras models are sequential.\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Add the list of features and how they are represented\n",
    "    model.add(featureLayer)\n",
    "    # Use a sigmoid activation function to funnel\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,), activation=tf.sigmoid),)\n",
    "    # Compile method will construct the layers into a model that TensorFlow can execute\n",
    "    # RMSprop is used to maintain a moving average of the square of the gradients, then divide the gradient by the root of the average\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=myLearningRate), loss=tf.keras.losses.binary_crossentropy, metrics=myMetrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed in a dataset into the model to train it\n",
    "# Epoch is a run through of the whole dataset, if we batch size then we only pass through parts of the dataset at a time\n",
    "# Epoch and batchSize will be changed to reduce the error rate\n",
    "def trainModel(model, dataset, epochs, labelName, batchSize = None, shuffle = True):\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label = np.array(features.pop(labelName))\n",
    "    # x paramter can be an array of the data for each feature. Feature layer will be filtering out the columns we don't want\n",
    "    history = model.fit(x=features, y=label, batch_size=batchSize, epochs=epochs, shuffle=shuffle)\n",
    "    # Can see the output of the model being trained over time\n",
    "    epochs = history.epochs\n",
    "    # Isolate the classification metric for each epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trained model against random training examples using MatPlotLib\n",
    "def plotModel(trainedWeight, trainedBias, feature, label):\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(label)\n",
    "    randomExamples = training_df.sample(n=200)\n",
    "    plt.scatter(randomExamples[feature], randomExamples[label])\n",
    "\n",
    "    x0 = 0\n",
    "    y0 = trainedBias\n",
    "    x1 = 10000\n",
    "    y1 = trainedBias + (trainedWeight * x1)\n",
    "    plt.plot([x0, x1], [y0, y1], c=\"r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and plot the loss curve, which will show how the model performs over epochs\n",
    "def plotLossCurve(epochs, rmse):\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.plot(epochs, rmse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([rmse.min()*0.97, rmse.max()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'smoothness_mean': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float32>, 'compactness_mean': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float32>, 'concavity_mean': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float32>, 'concave points_mean': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float32>, 'symmetry_mean': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float32>, 'fractal_dimension_mean': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float32>, 'radius_mean': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float32>, 'texture_mean': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float32>, 'perimeter_mean': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float32>, 'area_mean': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float32>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'dict'> input: {'smoothness_mean': <tf.Tensor 'ExpandDims_7:0' shape=(None, 1) dtype=float32>, 'compactness_mean': <tf.Tensor 'ExpandDims_1:0' shape=(None, 1) dtype=float32>, 'concavity_mean': <tf.Tensor 'ExpandDims_3:0' shape=(None, 1) dtype=float32>, 'concave points_mean': <tf.Tensor 'ExpandDims_2:0' shape=(None, 1) dtype=float32>, 'symmetry_mean': <tf.Tensor 'ExpandDims_8:0' shape=(None, 1) dtype=float32>, 'fractal_dimension_mean': <tf.Tensor 'ExpandDims_4:0' shape=(None, 1) dtype=float32>, 'radius_mean': <tf.Tensor 'ExpandDims_6:0' shape=(None, 1) dtype=float32>, 'texture_mean': <tf.Tensor 'ExpandDims_9:0' shape=(None, 1) dtype=float32>, 'perimeter_mean': <tf.Tensor 'ExpandDims_5:0' shape=(None, 1) dtype=float32>, 'area_mean': <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=float32>}\n",
      "Consider rewriting this model with the Functional API.\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.6825 - Accuracy: 0.5516\n",
      "Epoch 2/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.6549 - Accuracy: 0.6291\n",
      "Epoch 3/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.6436 - Accuracy: 0.6291\n",
      "Epoch 4/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.6353 - Accuracy: 0.6291\n",
      "Epoch 5/20\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.6281 - Accuracy: 0.6291\n",
      "Epoch 6/20\n",
      "43/43 [==============================] - 0s 3ms/step - loss: 0.6214 - Accuracy: 0.6291\n",
      "Epoch 7/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.6149 - Accuracy: 0.6291\n",
      "Epoch 8/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.6090 - Accuracy: 0.6291\n",
      "Epoch 9/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.6027 - Accuracy: 0.6291\n",
      "Epoch 10/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5962 - Accuracy: 0.6315\n",
      "Epoch 11/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5889 - Accuracy: 0.6432\n",
      "Epoch 12/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5819 - Accuracy: 0.6455\n",
      "Epoch 13/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5758 - Accuracy: 0.6502\n",
      "Epoch 14/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5704 - Accuracy: 0.6620\n",
      "Epoch 15/20\n",
      "43/43 [==============================] - 0s 1ms/step - loss: 0.5656 - Accuracy: 0.6667\n",
      "Epoch 16/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5602 - Accuracy: 0.6690\n",
      "Epoch 17/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5551 - Accuracy: 0.6901\n",
      "Epoch 18/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5504 - Accuracy: 0.6831\n",
      "Epoch 19/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5456 - Accuracy: 0.7089\n",
      "Epoch 20/20\n",
      "43/43 [==============================] - 0s 2ms/step - loss: 0.5412 - Accuracy: 0.7019\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'epochs'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-4fb75d28333b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassificationThreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmyModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m epochs, hist = trainModel(myModel, train_df, epochs, \n\u001b[0m\u001b[1;32m     11\u001b[0m                            labelName, batchSize)\n\u001b[1;32m     12\u001b[0m \u001b[0mmetricsToPlot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-1fb349871db1>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, dataset, epochs, labelName, batchSize, shuffle)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Can see the output of the model being trained over time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Isolate the classification metric for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'History' object has no attribute 'epochs'"
     ]
    }
   ],
   "source": [
    "# Invoke the functions\n",
    "learningRate = 0.01\n",
    "epochs = 20\n",
    "batchSize = 10\n",
    "labelName = \"diagnosis\"\n",
    "classificationThreshold = 0.5\n",
    "\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy(name=\"Accuracy\", threshold=classificationThreshold)]\n",
    "myModel = createModel(learningRate, featureLayer, metrics)\n",
    "epochs, hist = trainModel(myModel, train_df, epochs, \n",
    "                           labelName, batchSize)\n",
    "metricsToPlot = ['accuracy']\n",
    "#plotLossCurve(epochs, hist, metricsToPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}