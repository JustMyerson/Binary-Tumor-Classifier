{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensorflow version 2.3.1\npandas 1.0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import feature_column\n",
    "from matplotlib import pyplot as plt\n",
    "# Serves to check that the Tensor Flow version is correct and imported correctly\n",
    "print('tensorflow version', tf.__version__)\n",
    "print('pandas', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and create a dataframe.\n",
    "# Create a columns array\n",
    "columns = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n",
    "featureColumn = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n",
    "# Later we will need to split the data set into validation, test and training\n",
    "df = pd.read_csv('Duplicate.csv', names=columns, index_col=False)\n",
    "# Drop the ID column which we do not need, as well as the redundant headings\n",
    "df = df.drop('id', 1)\n",
    "df = df.drop(0)\n",
    "# Need to convert the object values into floats and the diagnosis into a string\n",
    "df['diagnosis'] = df.apply(lambda x: np.array(x['diagnosis']).astype(str), axis=1)\n",
    "for i in range(len(featureColumn)):\n",
    "    df[featureColumn[i]] = df.apply(lambda x: np.array(x[featureColumn[i]]).astype(float), axis=1)\n",
    "# Need to convert the Malignant or Bengin result into a binary value\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 0, 'B':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data over all the values, besides for target value\n",
    "# Calculate the z score\n",
    "# All rows, columns between 1 and 5\n",
    "#Normalising\n",
    "print(df.iloc[:,1:5].shape)\n",
    "print(df.iloc[:,1:5].mean().shape)\n",
    "dfNormalized = df.iloc[:,1:5].sub([df.iloc[:,1:5].mean()], axis=1)\n",
    "dfNormalized = df.div(df_std, axis=1)\n",
    "print(dfNormalized)\n",
    "\n",
    "#reassigning to dataframe\n",
    "#if rerunning, need to reimport data. the drop function permenantly changes dataframe meaning it can only operate once\n",
    "dfTemp = df\n",
    "dfTemp = dfTemp.drop(columns = [\"radius_mean\",  \"texture_mean\",  \"perimeter_mean\",  \"area_mean\"]) #colums 1,2,3,4\n",
    "df = pd.concat([dfTemp, dfNormalized], axis = 1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data. Use a 75% training and 25% test split\n",
    "# test_size is how much of the data will be used for testing\n",
    "# random_state controls the shuffling to tae place, by defining the random state we can reproduce the same split of the data across multiple function calls\n",
    "# shuffle, we will enable this as the data may be organised in a certain way which could affect our results\n",
    "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42, shuffle=True)\n",
    "test_df, validation_df = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "# Define path for test and training data to be saved and convert them to CSV\n",
    "train_path = Path('train.tsv')\n",
    "test_path = Path('test.tsv')\n",
    "validate_path = Path('validate.tsv')\n",
    "train_df.to_csv(train_path, sep='\\t', index=False)\n",
    "test_df.to_csv(test_path, sep='\\t', index=False)\n",
    "validation_df.to_csv(validate_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_norm = test_df.iloc[:,1:5].sub(test_df_mean, axis=1)\n",
    "test_df_norm = test_df_norm.div(test_df_std, axis=1)\n",
    "\n",
    "#reassigning to dataframe\n",
    "#if rerunning, need to reimport data. the drop function permenantly changes dataframe meaning it can only operate once\n",
    "test_df_temp = test_df\n",
    "test_df_temp = test_df_temp.drop(columns = [\"radius_mean\",  \"texture_mean\",  \"perimeter_mean\",  \"area_mean\"])\n",
    "test_df = pd.concat([test_df_temp, test_df_norm], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n288          1    -0.351099     -1.434456       -0.414792  -0.394952   \n513          0    -0.206380      0.286059       -0.137003  -0.279014   \n403          1    -0.331236     -0.232420       -0.320550  -0.368525   \n447          0     1.027996      2.032150        1.042470   0.928382   \n211          0     1.831050      0.662713        1.758550   1.804733   \n..         ...          ...           ...             ...        ...   \n72           1    -1.486725     -1.081053       -1.365450  -1.167583   \n107          1    -0.705805     -0.223120       -0.691347  -0.688773   \n271          1     0.046171     -0.574199       -0.068687  -0.063337   \n436          0    -0.041796      0.076807       -0.034941  -0.157394   \n103          1    -0.552572      0.286059       -0.606982  -0.557491   \n\n     smoothness_mean  compactness_mean  concavity_mean  concave_points_mean  \\\n288          0.06955           0.03729         0.02260              0.01171   \n513          0.11060           0.14690         0.14450              0.08172   \n403          0.07351           0.07899         0.04057              0.01883   \n447          0.09997           0.13140         0.16980              0.08293   \n211          0.09090           0.13480         0.16400              0.09561   \n..               ...               ...             ...                  ...   \n72           0.09783           0.15310         0.08606              0.02872   \n107          0.11420           0.10170         0.07070              0.03485   \n271          0.06429           0.02675         0.00725              0.00625   \n436          0.10600           0.11330         0.11260              0.06463   \n103          0.08013           0.04038         0.02383              0.01770   \n\n     symmetry_mean  fractal_dimension_mean  \n288         0.1337                 0.05581  \n513         0.2116                 0.07325  \n403         0.1874                 0.05899  \n447         0.1713                 0.05916  \n211         0.1765                 0.05024  \n..             ...                     ...  \n72          0.1902                 0.08980  \n107         0.1801                 0.06520  \n271         0.1508                 0.05376  \n436         0.1669                 0.06544  \n103         0.1739                 0.05677  \n\n[426 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the paramaters and add them to the feature vecture which will be used to train the model\n",
    "featureColumns = []\n",
    "for i in featureColumn:\n",
    "    featureColumns.append(tf.feature_column.numeric_column(i))\n",
    "featureLayer = tf.keras.layers.DenseFeatures(featureColumns)\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model topography, creating a simple classification model\n",
    "def createModel(myLearningRate, featureLayer, myMetrics):\n",
    "    # Most simple tf.keras models are sequential.\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Add the list of features and how they are represented\n",
    "    model.add(featureLayer)\n",
    "    # Use a sigmoid activation function to funnel\n",
    "    model.add(tf.keras.layers.Dense(units=1, input_shape=(1,), activation=tf.sigmoid),)\n",
    "    # Compile method will construct the layers into a model that TensorFlow can execute\n",
    "    # RMSprop is used to maintain a moving average of the square of the gradients, then divide the gradient by the root of the average\n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=myLearningRate), loss=tf.keras.losses.binary_crossentropy, metrics=myMetrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed in a dataset into the model to train it\n",
    "# Epoch is a run through of the whole dataset, if we batch size then we only pass through parts of the dataset at a time\n",
    "# Epoch and batchSize will be changed to reduce the error rate\n",
    "def trainModel(model, dataset, epochs, labelName, batchSize = None, shuffle = True):\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label = np.array(features.pop(labelName))\n",
    "    # x paramter can be an array of the data for each feature. Feature layer will be filtering out the columns we don't want\n",
    "    history = model.fit(x=features, y=label, batch_size=batchSize, epochs=epochs, shuffle=shuffle)\n",
    "    # Can see the output of the model being trained over time\n",
    "    epochs = history.epoch\n",
    "    # Isolate the classification metric for each epoch\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    return epochs, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a plotting function in order to visualize the training of the model\n",
    "def plotCurve(epochs, history, metrics):\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    for m in metrics:\n",
    "        x = history[m]\n",
    "        plt.plot(epochs[1:], x[1:], label = m)\n",
    "        \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trained model against random training examples using MatPlotLib\n",
    "def plotModel(trainedWeight, trainedBias, feature, label):\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(label)\n",
    "    randomExamples = training_df.sample(n=200)\n",
    "    plt.scatter(randomExamples[feature], randomExamples[label])\n",
    "\n",
    "    x0 = 0\n",
    "    y0 = trainedBias\n",
    "    x1 = 10000\n",
    "    y1 = trainedBias + (trainedWeight * x1)\n",
    "    plt.plot([x0, x1], [y0, y1], c=\"r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and plot the loss curve, which will show how the model performs over epochs\n",
    "def plotLossCurve(epochs, rmse):\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.plot(epochs, rmse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([rmse.min()*0.97, rmse.max()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'relu'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3461b8ce8da7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m metrics = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classificationThreshold), tf.keras.metrics.Precision(thresholds = classificationThreshold, name = 'precision'), tf.keras.metrics.Recall(thresholds = classificationThreshold,\n\u001b[1;32m     12\u001b[0m                               name =  'recall')]\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmyModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearningRate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m epochs, hist = trainModel(myModel, train_df, epochs, \n\u001b[1;32m     15\u001b[0m                            labelName, batchSize)\n",
      "\u001b[0;32m<ipython-input-32-be64187d4a14>\u001b[0m in \u001b[0;36mcreateModel\u001b[0;34m(myLearningRate, featureLayer, myMetrics)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Use a sigmoid activation function to funnel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Compile method will construct the layers into a model that TensorFlow can execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# RMSprop is used to maintain a moving average of the square of the gradients, then divide the gradient by the root of the average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'relu'"
     ]
    }
   ],
   "source": [
    "# Invoke the functions\n",
    "# Accuracy, how well predicts the label\n",
    "# Precision, divides true positives by false positives\n",
    "# Recall, divides true positives by sum of true positives and false positives\n",
    "learningRate = 0.1\n",
    "epochs = 75\n",
    "batchSize = 5\n",
    "labelName = \"diagnosis\"\n",
    "classificationThreshold = 0.5\n",
    "\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=classificationThreshold), tf.keras.metrics.Precision(thresholds = classificationThreshold, name = 'precision'), tf.keras.metrics.Recall(thresholds = classificationThreshold,\n",
    "                              name =  'recall')]\n",
    "myModel = createModel(learningRate, featureLayer, metrics)\n",
    "epochs, hist = trainModel(myModel, train_df, epochs, \n",
    "                           labelName, batchSize)\n",
    "metricsToPlot = ['accuracy', 'precision', 'recall']\n",
    "plotCurve(epochs, hist, metricsToPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}